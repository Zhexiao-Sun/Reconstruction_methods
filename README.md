<div align="center">
<img src="assets/imgs/icon.png" width="90"/>
</div>

<h1 align="center">Target-Bench:</h1>
<h3 align="center">Can World Models Achieve Mapless Path Planning with Semantic Targets?</h3>

<div align='center'>

ğŸ¯ [Project Page](https://target-bench.github.io/) â€¢ ğŸ“„ [Paper](https://target-bench.github.io/resources/TargetBench.pdf) â€¢ ğŸ¤— [Dataset](https://huggingface.co/target-bench) 

![teaser](assets/imgs/teaser.gif)

<br/>

</div>

[**TL;DR**] Target-Bench is the first benchmark and dataset for evaluating video world models (WMs) on mapless robotic path planning for semantic targets.

If you find our work useful, please star â­ our repo!

## TODO &#128203;<a name="todo"></a>
- [x] Fine-tune code release
- [x] Benchmark code release
- [x] Dataset release
- [x] Paper release
- [x] Website launch

## Installation

### 1. Clone the repository

```bash
git clone https://github.com/TUM-AVS/target-bench.git
cd target-bench
```

### 2. Environment Setup

Ensure you have [miniconda](https://docs.anaconda.com/free/miniconda/miniconda-install/) installed.

You can set up all environments at once or individually. For a quick start with VGGT:

```bash
# Install VGGT environment
bash set_env.sh vggt
```

For other options (installing all environments or specific ones like SpaTracker/ViPE), please refer to [docs/env.md](docs/env.md).

### 3. Dataset Download

Download the `benchmark_data` (scenarios) and `wm_videos` (generated videos) into the `dataset/` directory:

```bash
cd dataset

# Download Benchmark scenarios
huggingface-cli download target-bench/benchmark_data --repo-type dataset --local-dir Benchmark --local-dir-use-symlinks False

# Download World Model generated videos
huggingface-cli download target-bench/wm_videos --repo-type dataset --local-dir wm_videos --local-dir-use-symlinks False

cd ..
```

Now, the project directory structure should look like this:

```text
target-bench/
â”œâ”€â”€ assets/                  # Images and project assets
â”œâ”€â”€ dataset/                 # Benchmark data and generated videos
â”‚   â”œâ”€â”€ Benchmark/           # Benchmark scenarios
â”‚   â””â”€â”€ wm_videos/           # Videos generated by world models
â”œâ”€â”€ evaluation/              # Evaluation scripts and configs
â”œâ”€â”€ models/                  # Source code for evaluated models
â”‚   â”œâ”€â”€ spatracker/
â”‚   â”œâ”€â”€ vggt/
â”‚   â””â”€â”€ vipe/
â””â”€â”€ pipelines/               # World decoders adapted for each model
    â”œâ”€â”€ spatracker/
    â”œâ”€â”€ vggt/
    â””â”€â”€ vipe/
```

## Usage
Run a quick evaluation with 3 scenes using VGGT as the spatial-temporal tool:
```bash
conda activate vggt
cd evaluation
python target_eval_vggt.py -n 3 
```

Then you should be able to see the evaluation results and visualizations in the `evaluation_results` folder: 
![Trajectory Plot](./assets/imgs/trajectory_viz.png)

## Citation

```bibtex
@article{wang2025target,
  title={Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?},
  author={Wang, Dingrui and Ye, Hongyuan and Liang, Zhihao and Sun, Zhexiao and Lu, Zhaowei and Zhang, Yuchen and Zhao, Yuyu and Gao, Yuan and Seegert, Marvin and Sch{\"a}fer, Finn and others},
  journal={arXiv preprint arXiv:2511.17792},
  year={2025}
}
```

## Credits

This project builds upon the following open-source works:
- **[VGGT](https://github.com/facebookresearch/vggt)**
- **[ViPE](https://github.com/nv-tlabs/vipe)**
- **[SpatialTrackerV2](https://github.com/henry123-boy/SpaTrackerV2)**

Please refer to their respective directories for detailed credits and license information.
