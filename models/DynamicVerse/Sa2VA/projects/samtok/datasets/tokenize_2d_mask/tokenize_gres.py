import os
import random
import json
from PIL import Image
import numpy as np
from pycocotools import mask as mask_utils
import torch
import tqdm
import torchvision

from .grefer import G_REFER
from projects.samtok.models import VQ_SAM2, VQ_SAM2Config, SAM2Config, DirectResize


SEG_QUESTIONS = [
    "Can you segment the {class_name} in this image?",
    "Please segment {class_name} in this image.",
    "What is {class_name} in this image? Please respond with segmentation mask.",
    "What is {class_name} in this image? Please output segmentation mask.",

    "Can you segment the {class_name} in this image",
    "Please segment {class_name} in this image",
    "What is {class_name} in this image? Please respond with segmentation mask",
    "What is {class_name} in this image? Please output segmentation mask",

    "Could you provide a segmentation mask for the {class_name} in this image?",
    "Please identify and segment the {class_name} in this image.",
    "Where is the {class_name} in this picture? Please respond with a segmentation mask.",
    "Can you highlight the {class_name} in this image with a segmentation mask?",

    "Could you provide a segmentation mask for the {class_name} in this image",
    "Please identify and segment the {class_name} in this image",
    "Where is the {class_name} in this picture? Please respond with a segmentation mask",
    "Can you highlight the {class_name} in this image with a segmentation mask",
]

ANSWER_LIST = [
    "It is {SEG}.",
    "Sure, {SEG}.",
    "Sure, it is {SEG}.",
    "Sure, the segmentation result is {SEG}.",
    "{SEG}.",
]

NO_TARGETS_ANSWER_LIST = [
    "No target."
]

def decode_mask(object_masks, ori_height, ori_width):
    binary_masks = []
    for object_mask in object_masks:
        if isinstance(object_mask, dict):
            if isinstance(object_mask["counts"], list):
                # convert to compressed RLE
                object_mask = mask_utils.frPyObjects(object_mask, ori_height, ori_width)
            m = mask_utils.decode(object_mask)
            m = m.astype(np.uint8).squeeze()
        elif object_mask:
            rles = mask_utils.frPyObjects(object_mask, ori_height, ori_width)
            rle = mask_utils.merge(rles)
            m = mask_utils.decode(rle).astype(np.uint8).squeeze()
        else:
            m = np.zeros((ori_height, ori_width), dtype=np.uint8)
        binary_masks.append(m)
    return binary_masks

def mask_iou(mask1, mask2):
    mask1 = mask1.unsqueeze(1).char() # n, 1, h, w
    mask2 = mask2.unsqueeze(0).char() # 1, n, h, w

    intersection = (mask1 & mask2)
    union = (mask1 + mask2 - intersection).sum(-1).sum(-1)
    intersection = intersection.sum(-1).sum(-1)

    return intersection / union

def sort_mask_indices(masks_t: torch.Tensor, mode: str = "ltr-ttb") -> np.ndarray:
    """
    根据实例的几何位置给出排序索引。
    Args:
        masks_t: [N, H, W] 的 torch.bool/uint8 张量（每个实例一个二值mask）
        mode:
            - "ltr-ttb": left-to-right, then top-to-bottom（先按x中心，再按y中心）
            - "ttb-ltr": top-to-bottom, then left-to-right（先按y中心，再按x中心）
            - "tlbr":    purely by top-left (y1, x1) 先y后x（行优先）
    Returns:
        order: numpy 数组，形状 [N]，是重排索引
    """
    # 利用bbox/中心点作为排序依据（更稳定、无需遍历像素）
    boxes = torchvision.ops.masks_to_boxes(masks_t)  # [N,4] (x1,y1,x2,y2)
    x1, y1, x2, y2 = boxes.unbind(dim=1)
    xc = ((x1 + x2) * 0.5).cpu().numpy()
    yc = ((y1 + y2) * 0.5).cpu().numpy()
    y1n = y1.cpu().numpy()
    x1n = x1.cpu().numpy()

    if mode == "ltr-ttb":
        # 先x后y：主键x_center，次键y_center
        order = np.lexsort((yc, xc))
    elif mode == "ttb-ltr":
        # 先y后x：主键y_center，次键x_center
        order = np.lexsort((xc, yc))
    elif mode == "tlbr":
        # 以bbox左上角先y后x（更像“逐行”）
        order = np.lexsort((x1n, y1n))
    else:
        raise ValueError(f"Unknown mode: {mode}")
    return order

def main():

    # build mask tokenizer
    MT_START_TOKEN = '<|mt_start|>'
    MT_END_TOKEN = '<|mt_end|>'
    MT_CONTEXT_TOKEN = '<|mt_{}|>'

    temp_save_root = "./temp_data_256x2/ref_seg/gres"
    if not os.path.exists(temp_save_root):
        os.makedirs(temp_save_root)
    dataset_name = "gres"
    
    sam2_config = SAM2Config(
        cfg_path="sam2.1_hiera_l.yaml",
        ckpt_path="zhouyik/Qwen3-VL-8B-SAMTok/sam2.1_hiera_large.pt",
    )
    CODEBOOK_SIZE = 256
    CODEBOOK_DEPTH = 2
    vq_sam2_config = VQ_SAM2Config(
        sam2_config=sam2_config,
        codebook_size=CODEBOOK_SIZE,
        codebook_depth=CODEBOOK_DEPTH,
        shared_codebook=False,
        latent_dim=256,
    )
    vq_sam2 = VQ_SAM2(vq_sam2_config).cuda().eval()
    state = torch.load("zhouyik/Qwen3-VL-8B-SAMTok/mask_tokenizer_256x2.pth", map_location="cpu")
    vq_sam2.load_state_dict(state)

    sam2_image_processor = DirectResize(1024)
    
    # load dataset
    refer_api = G_REFER('./data/ref_seg/grefs/coco2014/train2014', './data/ref_seg/grefs/grefs(unc).json', './data/ref_seg/grefs/instances.json')
    ref_ids_train = refer_api.getRefIds(split="train")
    images_ids_train = refer_api.getImgIds(ref_ids=ref_ids_train)
    refs_train = refer_api.loadRefs(ref_ids=ref_ids_train)
    refer_seg_ds = {}
    refer_seg_ds["images"] = []
    loaded_images = refer_api.loadImgs(image_ids=images_ids_train)

    for item in loaded_images:
        item = item.copy()
        item["file_name"] = os.path.join('./data/ref_seg/grefs/coco2014/train2014', item["file_name"])
        refer_seg_ds["images"].append(item)
    refer_seg_ds["annotations"] = refer_api.Anns  # anns_train

    print("dataset {} (refs {}) (train split) has {} images and {} annotations.".format('grefcoco', 'unc', len(refer_seg_ds["images"]), len(refer_seg_ds["annotations"])))

    img2refs = {}
    for ref in refs_train:
        image_id = ref["image_id"]
        img2refs[image_id] = img2refs.get(image_id, []) + [ref]
    refer_seg_ds["img2refs"] = img2refs
    
    # tokenize 2d mask
    count = 0
    shard_size = 10000
    shard_items = []
    shard_idx = 0
    for index in tqdm.tqdm(range(len(refer_seg_ds['images']))):
        image_info = refer_seg_ds["images"][index]
        image_path = image_info["file_name"]
        image_id = image_info["id"]
        refs = img2refs[image_id]
        if len(refs) == 0:
            continue

        sents = []
        ann_ids = []
        for ref in refs:
            for sent in ref["sentences"]:
                text = sent["sent"]
                sents.append(text)
                ann_ids.append(ref["ann_id"])
        
        anno_masks = []
        for ann_id in ann_ids:
            if isinstance(ann_id, list):
                if -1 in ann_id:
                    assert len(ann_id) == 1
                    m = [np.zeros((image_info["height"], image_info["width"])).astype(np.uint8)]
                else:
                    m_list = []
                    for ann_id_i in ann_id:
                        ann = refer_seg_ds["annotations"][ann_id_i]
                        if len(ann["segmentation"]) == 0:
                            m = np.zeros((image_info["height"], image_info["width"])).astype(np.uint8)
                        else:
                            if isinstance(ann["segmentation"], dict):
                                rle = ann["segmentation"]
                                assert isinstance(rle["counts"], list)
                                # convert to compressed RLE
                                rle = mask_utils.frPyObjects(rle, image_info["height"], image_info["width"])
                            else:
                                rle = mask_utils.frPyObjects(
                                    ann["segmentation"],
                                    image_info["height"],
                                    image_info["width"],
                                )
                            m = mask_utils.decode(rle)
                            if m.ndim < 3:
                                assert m.ndim == 2
                                m = m[..., np.newaxis]
                            m = np.sum(m, axis=2).astype(np.uint8)  # convert to np.uint8
                            m = m
                        m_list.append(m)
                    m = m_list
                anno_masks.append(m)
                continue

            ann = refer_seg_ds["annotations"][ann_id]

            if len(ann["segmentation"]) == 0:
                m = [np.zeros((image_info["height"], image_info["width"])).astype(np.uint8)]
                anno_masks.append(m)
                continue

            if isinstance(ann["segmentation"][0], list):  # polygon
                rle = mask_utils.frPyObjects(ann["segmentation"], image_info["height"], image_info["width"])
            else:
                rle = ann["segmentation"]
                for i in range(len(rle)):
                    if not isinstance(rle[i]["counts"], bytes):
                        rle[i]["counts"] = rle[i]["counts"].encode()
            m = mask_utils.decode(rle)
            # m = np.sum(m, axis=2)  # sometimes there are multiple binary map (corresponding to multiple segs)
            m = [m[:, :, _i_].astype(np.uint8) for _i_ in range(m.shape[2])]
            anno_masks.append(m)

        image = Image.open(image_path).convert('RGB')
        ori_width, ori_height = image.size

        sam2_image = np.array(image)
        sam2_image = sam2_image_processor.apply_image(sam2_image)
        sam2_pixel_values = torch.from_numpy(sam2_image).permute(2, 0, 1).contiguous()
        sam2_pixel_values = sam2_pixel_values.unsqueeze(0).to(vq_sam2.dtype).to(vq_sam2.device)

        for sent, binary_masks in zip(sents, anno_masks):
            if len(binary_masks) == 1 and np.sum(binary_masks[0]) < 1.0:
                # No Target
                question = random.choice(SEG_QUESTIONS).format(class_name=sent)
                question = "<image>\n" + question
                answer = random.choice(NO_TARGETS_ANSWER_LIST)
                conversation = []
                conversation.append({'from': 'human', 'value': question})
                conversation.append({'from': 'gpt', 'value': answer})
                ret_data_dict = {
                    'image': image_path,
                    'conversations': conversation,
                }
                shard_items.append(ret_data_dict)
                count += 1
                
                if count % shard_size == 0:
                    shard_idx += 1
                    out_path = os.path.join(temp_save_root, f"{dataset_name}-segment-{shard_idx:05d}.json")
                    with open(out_path, "w") as f:
                        json.dump(shard_items, f)
                    shard_items.clear()
                    print(f"[SAVE] {out_path} ({count} items)", flush=True)
                continue
            
            masks = torch.stack([torch.from_numpy(np.ascontiguousarray(x.copy())) for x in binary_masks])
            try:
                order = sort_mask_indices(masks, mode="ltr-ttb")
            except:
                order = np.arange(masks.shape[0])

            masks = masks[torch.as_tensor(order, dtype=torch.long)]

            try:
                boxes = torchvision.ops.masks_to_boxes(masks)
            except:
                print("Error at boxes = torchvision.ops.masks_to_boxes(masks)")
                continue

            whwh = torch.as_tensor([[ori_width, ori_height, ori_width, ori_height]])
            boxes = boxes / whwh
            boxes = boxes.to(vq_sam2.device)
            masks = [m.unsqueeze(0).to(vq_sam2.device) for m in masks]
            num_ins = len(masks)
            
            with torch.no_grad():
                vq_sam2_output = vq_sam2(
                    sam2_pixel_values.repeat(num_ins, 1, 1, 1),
                    masks,
                    boxes,
                    reconstruct_mask=False,
                )
                quant_codes = vq_sam2_output.quant_codes
            
            quant_codes = quant_codes.cpu().numpy().astype(np.int32).tolist()
            remap_quant_codes = []
            for _quant_codes in quant_codes:
                _quant_codes = _quant_codes[0]
                remap_quant_codes.append([depth_idx*CODEBOOK_SIZE+quant_code for depth_idx, quant_code in enumerate(_quant_codes)])
            quant_codes = remap_quant_codes

            question = random.choice(SEG_QUESTIONS).format(class_name=sent)
            question = "<image>\n" + question

            answer = "```json\n[{mask_2d}]\n```"
            mask_2d_str = ''
            for _quant_codes_ in quant_codes:
                sam2_tokens = MT_START_TOKEN + ''.join([MT_CONTEXT_TOKEN.format(str(code).zfill(4)) for code in _quant_codes_]) + MT_END_TOKEN
                item_str = "{\"mask_2d\": " + sam2_tokens + ", \"label\": \"one of the " + sent + "\"}"
                mask_2d_str += item_str + ",\n"
            mask_2d_str = mask_2d_str[:-len(",\n")]
            answer = answer.format(mask_2d=mask_2d_str)

            conversation = []
            conversation.append({'from': 'human', 'value': question})
            conversation.append({'from': 'gpt', 'value': answer})

            ret_data_dict = {
                'image': image_path,
                'conversations': conversation,
            }
            shard_items.append(ret_data_dict)
            count += 1

            if count % shard_size == 0:
                shard_idx += 1
                out_path = os.path.join(temp_save_root, f"{dataset_name}-segment-{shard_idx:05d}.json")
                with open(out_path, "w") as f:
                    json.dump(shard_items, f)
                shard_items.clear()
                print(f"[SAVE] {out_path} ({count} items)", flush=True)

    if shard_items:
        shard_idx += 1
        out_path = os.path.join(temp_save_root, f"{dataset_name}-segment-{shard_idx:05d}.json")
        with open(out_path, "w") as f:
            json.dump(shard_items, f)
        shard_items.clear()
        print(f"[SAVE] {out_path} (final, total={count})", flush=True) 

if __name__ == "__main__":
    main()


