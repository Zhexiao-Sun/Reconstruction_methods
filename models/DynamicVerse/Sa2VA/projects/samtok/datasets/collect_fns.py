from typing import Dict, Sequence

import numpy as np
import torch
from torch.nn.utils.rnn import pad_sequence

from xtuner.parallel.sequence import (get_sequence_parallel_world_size,
                                      pad_for_sequence_parallel)
from xtuner.utils import DEFAULT_PAD_TOKEN_INDEX, IGNORE_INDEX

def perceptionlm_collate_fn(instances: Sequence[Dict],
                        pad_index: int = DEFAULT_PAD_TOKEN_INDEX,
                        return_hf_format: bool = False,
                        use_varlen_attn: bool = False):
    seq_parallel_world_size = get_sequence_parallel_world_size()

    input_ids, labels = [], []
    has_image = any(inst.get('pixel_values') is not None for inst in instances)
    has_aspect_ratio = any(inst.get('aspect_ratio') is not None for inst in instances)
    if use_varlen_attn:
        position_ids, cumulative_len = [], []
        assert len(instances) == 1, (
            f'If utilizing varlen attention, the batch size should be'
            f' set to 1, but got {len(instances)}')
        assert not has_image, 'Currently, it is not configured to '
        'accommodate the use of varlen Attention in multimodal training'
    
    if has_image:
        pixel_values = []
    if has_aspect_ratio:
        aspect_ratios = []
        bboxes = []
    
    first = instances[0]
    for example in instances:
        input_ids.append(torch.LongTensor(example['input_ids']))
        labels.append(torch.LongTensor(example['labels']))
        if use_varlen_attn:
            cumulative_len.append(torch.IntTensor(example['cumulative_len']))
            position_ids.append(torch.LongTensor(example['position_ids']))
        
        if has_image:
            # pixel_values.append(example['pixel_values'].squeeze())
            pixel_values.extend([pixel_values_i.squeeze(0) for pixel_values_i in example['pixel_values']])
        if has_aspect_ratio:
            aspect_ratios.append(example['aspect_ratio'])
            bboxes.append(example['bboxes'])
        
    ori_length = [len(ids) for ids in input_ids]
    if len(instances) > 1:
        input_ids = pad_sequence(
            input_ids, batch_first=True, padding_value=pad_index)
        labels = pad_sequence(
            labels, batch_first=True, padding_value=IGNORE_INDEX)
    else:
        input_ids = torch.stack(input_ids)
        labels = torch.stack(labels)
    
    if use_varlen_attn:
        assert input_ids.size(1) % seq_parallel_world_size == 0
        attention_mask = None
        position_ids = torch.stack(position_ids, dim=0)
    else:
        # Some tokenizers have the same eos token and pad token, so input_ids
        # cannot be masked directly based on the pad token id.
        attention_mask = torch.zeros_like(input_ids).bool()
        for i, length in enumerate(ori_length):
            attention_mask[i, :length] = True

        bs, seq_len = input_ids.shape
        position_ids = torch.arange(seq_len).unsqueeze(0).long().repeat(bs, 1)
    
    if seq_parallel_world_size > 1:
        input_ids = pad_for_sequence_parallel(input_ids, pad_index)
        labels = pad_for_sequence_parallel(labels, IGNORE_INDEX)
        position_ids = pad_for_sequence_parallel(position_ids, 0)
        if attention_mask is not None:
            attention_mask = pad_for_sequence_parallel(attention_mask, 0)
    
    if use_varlen_attn:
        max_seqlen = (
            cumulative_len[0][1:] -  # noqa: W504
            cumulative_len[0][:-1]).max().item()
        data_dict = {
            'input_ids': input_ids,
            'cumulative_len': cumulative_len,
            'position_ids': position_ids,
            'labels': labels,
            'max_seqlen': max_seqlen
        }
    else:
        data_dict = {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'position_ids': position_ids,
            'labels': labels
        }
    
    if has_image:
        try:
            data_dict['pixel_values'] = torch.cat(pixel_values, dim=0).unsqueeze(0)
        except:
            for idx, item in enumerate(pixel_values):
                print(f"======>>>{idx}: ", item.shape)
            exit(0)
    if has_aspect_ratio:
        data_dict['aspect_ratios'] = torch.cat(aspect_ratios, dim=0)

    for k, v in first.items():
        if k in ('ori_image_list'):
            pass
        else:
            if k not in ('image_flags', 'num_patches', 'num_vprompts', 'sampled_mark_token_ids', 
                        'pixel_values', 'visual_prompts', 'merged_visual_prompts', 
                        'global_mask_values', 'aspect_ratios', 'bboxes') and v is not None and not isinstance(v, str):
                if isinstance(v, torch.Tensor):
                    if all([example[k].size()==v.size() for example in instances]):
                        data_dict[k] = torch.stack([example[k] for example in instances])
                elif isinstance(v, np.ndarray):
                    if all([example[k].shape==first.shape] for example in instances):
                        data_dict[k] = torch.tensor(np.stack([example[k] for example in instances]))
                else:
                    data_dict[k] = torch.tensor([example[k] for example in instances])
            if k in ('image_flags', 'num_patches', 'num_vprompts', 'sampled_mark_token_ids'):
                if isinstance(v, torch.Tensor):
                    data_dict[k] = torch.cat([example[k] for example in instances])
                elif isinstance(v, np.ndarray):
                    data_dict[k] = torch.tensor(np.stack([example[k] for example in instances]))
                else:
                    data_dict[k] = torch.tensor([example[k] for example in instances])
    
    if return_hf_format:
        return data_dict
    else:
        return {'data': data_dict, 'data_samples': None}
    



PAD_TOKEN_ID = 151643
IGNORE_INDEX = -100
MODEL_MAX_LENGTH = 8192

def pad_and_cat(tensor_list):
    max_length = max(tensor.shape[2] for tensor in tensor_list)

    padded_tensors = []
    for tensor in tensor_list:
        pad_length = max_length - tensor.shape[2]
        padded_tensor = torch.nn.functional.pad(tensor, (0, pad_length), "constant", 1)
        padded_tensors.append(padded_tensor)

    stacked_tensor = torch.cat(padded_tensors, dim=1)

    return stacked_tensor

def qwen25vl_vqsam2_collate_fn(instances: Sequence[Dict]):
    input_ids, labels, position_ids, attention_mask = tuple(
        [instance[key] for instance in instances]
        for key in ("input_ids", "labels", "position_ids", "attention_mask")
    )
    input_ids = [ids.squeeze(0) for ids in input_ids]
    labels = [ids.squeeze(0) for ids in labels]
    input_ids = torch.nn.utils.rnn.pad_sequence(
        input_ids, batch_first=True, padding_value=PAD_TOKEN_ID
    )
    labels = torch.nn.utils.rnn.pad_sequence(
        labels, batch_first=True, padding_value=IGNORE_INDEX
    )
    position_ids = pad_and_cat(position_ids)
    input_ids = input_ids[:, :MODEL_MAX_LENGTH]
    labels = labels[:, :MODEL_MAX_LENGTH]
    position_ids = position_ids[:, :MODEL_MAX_LENGTH]
    batch = dict(
        input_ids=input_ids,
        labels=labels,
        attention_mask=input_ids.ne(PAD_TOKEN_ID),
    )
    images = list(
        instance["pixel_values"]
        for instance in instances
        if "pixel_values" in instance
    )
    videos = list(
        instance["pixel_values_videos"]
        for instance in instances
        if "pixel_values_videos" in instance
    )
    if len(images) != 0:
        concat_images = torch.cat([image for image in images], dim=0)
        grid_thw = [
            instance["image_grid_thw"]
            for instance in instances
            if "image_grid_thw" in instance
        ]
        grid_thw = torch.cat(grid_thw, dim=0)
    else:
        concat_images = None
        grid_thw = None

    if len(videos) != 0:
        concat_videos = torch.cat([video for video in videos], dim=0)
        video_grid_thw = [
            instance["video_grid_thw"]
            for instance in instances
            if "video_grid_thw" in instance
        ]
        video_grid_thw = torch.cat(video_grid_thw, dim=0)
    else:
        concat_videos = None
        video_grid_thw = None

    batch["pixel_values"] = concat_images
    batch["image_grid_thw"] = grid_thw
    batch["pixel_values_videos"] = concat_videos
    batch["video_grid_thw"] = video_grid_thw
    batch["position_ids"] = position_ids


    # handle sam2 inputs
    has_mask = any(inst.get('masks', None) is not None for inst in instances)
    if not has_mask:
        batch['masks'] = None
        batch['sam2_pixel_values'] = None
    else:
        sam2_pixel_values = []
        masks = []
        for example in instances:
            if example.get('sam2_pixel_values', None) is not None:
                sam2_pixel_values.extend(example['sam2_pixel_values'])
                masks.extend(example['masks'])
        batch['sam2_pixel_values'] = torch.stack(sam2_pixel_values, dim=0)
        batch['masks'] = masks
    
    return {'data': batch, 'data_samples': None}

def vq_sam2_collate_fn(instances: Sequence[Dict]):
    
    pixel_values = []
    # image_grid_thw = []
    # sam2_pixel_values = []
    masks = []
    boxes = []

    for example in instances:
        pixel_values.append(example['pixel_values'])
        # image_grid_thw.append(example['image_grid_thw'])
        # sam2_pixel_values.append(example['sam2_pixel_values'])
        if isinstance(example['masks'], list):
            if isinstance(example['masks'][0], np.ndarray):
                _masks = np.stack(example['masks'], axis=0)
                _masks = torch.from_numpy(_masks)
                masks.append(_masks)
            else:
                masks.append(torch.stack(example['masks'], dim=0))
        else:
            masks.append(example['masks'])

        boxes.append(example['boxes'])
    
    data_dict = {
        'pixel_values': torch.stack(pixel_values, dim=0),
        'masks': masks,
        'boxes': torch.cat(boxes, dim=0)
    }

    return {'data': data_dict, 'data_samples': None}